{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"pQaiuVLajSal","executionInfo":{"status":"error","timestamp":1720780466633,"user_tz":360,"elapsed":278,"user":{"displayName":"Caleb","userId":"16116987403068975514"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"6a2bef1d-97cb-409e-dd2e-f333e7ccc08f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'players' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b737c8ac56a2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Player.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'players' is not defined"]}],"source":["# @title Default title text\n","\n","years = list(range(1991,2022))\n","url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n","\n","import requests\n","import time\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","# format the url and use requests to retrieve and store html\n","# data from each url in loop\n","for year in years:\n","  url = url_start.format(year)\n","  time.sleep(15)\n","  data = requests.get(url)\n","\n","  with open(\"mvp/{}.html\".format(year), \"w+\") as f:\n","    f.write(data.text)\n","\n","\n","from bs4 import BeautifulSoup\n","\n","# Single year test\n","with open(\"mvp/1991.html\") as f:\n","  page = f.read()\n","\n","soup = BeautifulSoup(page, \"html.parser\")\n","soup.find('tr', class_=\"over_header\").decompose()\n","mvp_table = soup.find(id=\"mvp\")\n","\n","import pandas as pd\n","\n","mvp_1991 = pd.read_html(str(mvp_table))[0]\n","\n","\n","\n","# for each html file parse the file to find the tables with the id for mvp then convert it into a\n","# dataframe with pandas and append that to a list of dataframes then convert to csv\n","dfs = []\n","for year in years:\n","\n","  with open(\"mvp/{}.html\".format(year)) as f:\n","   page = f.read()\n","  soup = BeautifulSoup(page, \"html.parser\")\n","  soup.find('tr', class_=\"over_header\").decompose()\n","  mvp_table = soup.find(id=\"mvp\")\n","  mvp = pd.read_html(str(mvp_table))[0]\n","  mvp[\"Year\"] = year\n","  dfs.append(mvp)\n","\n","mvps = pd.concat(dfs)\n","\n","mvps.head()\n","\n","mvps.to_csv(\"mvps.csv\")\n","\n","\n","## Repeat above for player stats in each year\n","player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n","\n","url = player_stats_url.format(1991)\n","data = requests.get(url)\n","with open(\"player/1991.html\".format(year), \"w+\") as f:\n","   f.write(data.text)\n","\n","# Connect driver\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.chrome.options import Options\n","\n","options = Options()\n","\n","options.add_argument('--headless')  # Run in headless mode\n","options.add_argument('--no-sandbox')  # Bypass OS security model\n","options.add_argument('--disable-dev-shm-usage')  # Overcome limited resource problems\n","options.add_argument('--remote-debugging-port=9222')  # Use a specific port for debugging\n","options.add_argument('--disable-gpu')  # Disable GPU hardware acceleration\n","\n","service = Service()\n","driver = webdriver.Chrome(service=service, options=options)\n","\n","# single year test\n","year = 1991\n","\n","url = player_stats_url.format(year)\n","driver.get(url)\n","driver.execute_script(\"window.scrollTo(1,10000)\")\n","time.sleep(2)\n","\n","html = driver.page_source\n","\n","with open(\"player/{}.html\".format(year), \"w+\") as f:\n","  f.write(html)\n","\n","# Player stats table can fully load on the driver, so the player page's html\n","# can be stored in a loop\n","for year in years:\n","  url = player_stats_url.format(year)\n","  driver.get(url)\n","  driver.execute_script(\"window.scrollTo(1,10000)\")\n","  time.sleep(10)\n","\n","  html = driver.page_source\n","  with open(\"player/{}.html\".format(year), \"w+\") as f:\n","    f.write(html)\n","\n","# for each html parse the file to find the table with the id for per game stats then\n","# then convert it into a dataframe with pandas and append that to a list of dataframes then convert to csv\n","dfs = []\n","for year in years:\n"," with open(\"player/{}.html\".format(year)) as f:\n","  page = f.read()\n","\n"," soup = BeautifulSoup(page, \"html.parser\")\n"," soup.find('tr', class_=\"thead\").decompose()\n"," player_table = soup.find(id=\"per_game_stats\")\n"," player = pd.read_html(str(player_table))[0]\n"," player[\"Year\"] = year\n"," dfs.append(player)\n","\n","\n","\n","players = pd.concat(dfs)\n","players.to_csv(\"Player.csv\")\n","\n","# format the url and use requests to retrieve and store html\n","# data from each url in loop\n","team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n","for year in years:\n"," url = team_stats_url.format(year)\n"," data = requests.get(url)\n"," time.sleep(10)\n"," with open(\"team/{}.html\".format(year), \"w+\") as f:\n","  f.write(data.text)\n","\n","# for each html parse the file to find the table with the id for divs_standings then\n","# then convert it into a dataframe with pandas and append that to a list of dataframes then convert to csv\n","dfs = []\n","for year in years:\n","  with open(\"team/{}.html\".format(year)) as f:\n","    page = f.read()\n","  soup = BeautifulSoup(page, \"html.parser\")\n","  soup.find('tr', class_=\"thead\").decompose()\n","  team_table = soup.find(id=\"divs_standings_E\")\n","  team = pd.read_html(str(team_table))[0]\n","  team[\"Year\"] = year\n","  team[\"Team\"] = team[\"Eastern Conference\"]\n","  del team[\"Eastern Conference\"]\n","  dfs.append(team)\n","\n","  soup = BeautifulSoup(page, \"html.parser\")\n","  soup.find('tr', class_=\"thead\").decompose()\n","  team_table = soup.find(id=\"divs_standings_W\")\n","  team = pd.read_html(str(team_table))[0]\n","  team[\"Year\"] = year\n","  team[\"Team\"] = team[\"Western Conference\"]\n","  del team[\"Western Conference\"]\n","  dfs.append(team)\n","teams = pd.concat(dfs)\n","\n","teams.to_csv(\"teams.csv\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"22I8uKryGkfZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1nRkSSO3JrhHrbyTox9CHizCiGvUL7A6r","authorship_tag":"ABX9TyMU2a9m2H+/IEzklxjjDzlk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}